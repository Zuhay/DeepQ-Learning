{
 "cells": [
  {
   "cell_type": "raw",
   "id": "397e0334",
   "metadata": {},
   "source": [
    "DeepQ Learning: MountainCar example in Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d30d6d",
   "metadata": {},
   "source": [
    "# 1.Import the MountainCar environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surface-launch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "demographic-cannon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of state =  2\n",
      "No of actions = 3\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "print(\"Shape of state = \", env.observation_space.shape[0])\n",
    "print(\"No of actions =\", env.action_space.n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-joseph",
   "metadata": {},
   "source": [
    "# 2.  Try some random moves to provide a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "simplified-grocery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2> 0> 1> 2> 1> 1> 1> 1> 2> 1> 2> 0> 0> 0> 0> 2> 2> 1> 1> 2> 1> 2> 1> 0> 0> 1> 2> 2> 1> 0> 1> 1> 2> 1> 1> 1> 2> 0> 0> 1> 2> 1> 0> 1> 0> 0> 1> 1> 0> 0> 1> 2> 1> 2> 1> 2> 1> 1> 0> 2> 1> 0> 2> 0> 1> 0> 0> 0> 0> 0> 1> 2> 2> 1> 1> 0> 0> 0> 1> 1> 0> 1> 2> 0> 2> 0> 2> 0> 0> 0> 2> 0> 2> 1> 1> 0> 0> 1> 2> 2> 0> 0> 2> 0> 0> 2> 0> 1> 2> 2> 1> 0> 2> 0> 1> 0> 2> 1> 1> 2> 0> 1> 0> 1> 1> 2> 0> 1> 0> 0> 2> 1> 0> 1> 2> 0> 2> 1> 0> 2> 0> 1> 1> 2> 0> 1> 2> 2> 1> 1> 1> 1> 1> 2> 1> 0> 2> 1> 2> 2> 2> 0> 0> 1> 2> 0> 0> 2> 2> 1> 2> 1> 1> 0> 2> 0> 0> 1> 1> 0> 0> 1> 0> 2> 1> 2> 0> 2> 0> 0> 2> 1> 1> 0> 1> 0> 0> 2> 2> 0\n",
      " Episode 1 Total Reward = -200.0\n",
      "> 2> 1> 0> 1> 1> 2> 2> 2> 1> 0> 1> 0> 0> 0> 2> 0> 1> 0> 1> 2> 0> 0> 1> 0> 2> 2> 1> 2> 2> 0> 2> 2> 1> 2> 1> 2> 0> 2> 0> 2> 2> 2> 1> 0> 2> 1> 2> 1> 0> 1> 2> 2> 1> 0> 0> 1> 0> 0> 0> 2> 1> 2> 0> 1> 1> 2> 1> 2> 2> 2> 2> 1> 1> 2> 1> 2> 2> 2> 0> 0> 0> 0> 0> 2> 0> 0> 2> 0> 0> 1> 2> 0> 1> 2> 1> 1> 2> 0> 0> 2> 0> 2> 1> 1> 2> 1> 0> 0> 1> 0> 2> 1> 0> 0> 1> 1> 2> 0> 0> 0> 1> 1> 1> 1> 1> 2> 2> 0> 0> 1> 0> 1> 2> 1> 0> 1> 1> 0> 0> 2> 1> 0> 0> 2> 1> 2> 0> 2> 0> 1> 2> 0> 1> 1> 0> 0> 0> 0> 0> 2> 0> 2> 1> 1> 2> 2> 2> 2> 0> 0> 1> 2> 0> 2> 2> 1> 1> 2> 1> 2> 0> 2> 2> 1> 1> 2> 1> 2> 1> 2> 1> 1> 1> 0> 2> 0> 0> 2> 0> 1\n",
      " Episode 2 Total Reward = -200.0\n",
      "> 0> 0> 0> 0> 1> 0> 2> 1> 0> 2> 1> 0> 1> 0> 2> 0> 2> 1> 0> 2> 0> 1> 0> 0> 0> 2> 1> 2> 1> 1> 1> 2> 0> 1> 1> 2> 0> 0> 1> 2> 1> 0> 2> 0> 1> 1> 2> 2> 1> 2> 2> 2> 2> 2> 1> 0> 1> 0> 0> 2> 1> 0> 1> 2> 0> 0> 1> 1> 1> 0> 1> 2> 1> 2> 2> 0> 0> 0> 2> 2> 2> 2> 1> 1> 0> 0> 2> 1> 0> 1> 2> 1> 0> 1> 2> 0> 2> 2> 0> 2> 1> 1> 1> 0> 2> 0> 1> 1> 0> 1> 0> 2> 2> 0> 2> 0> 2> 2> 1> 2> 2> 0> 0> 0> 2> 2> 0> 0> 1> 0> 2> 0> 1> 1> 1> 0> 2> 2> 1> 2> 2> 2> 1> 2> 1> 1> 1> 1> 2> 1> 0> 2> 1> 2> 1> 1> 0> 0> 1> 0> 2> 1> 2> 2> 2> 2> 0> 2> 2> 1> 1> 2> 1> 1> 1> 0> 1> 1> 2> 0> 2> 0> 2> 0> 0> 1> 0> 0> 0> 1> 0> 1> 0> 0> 0> 0> 1> 0> 1> 2\n",
      " Episode 3 Total Reward = -200.0\n",
      "> 0> 1> 0> 1> 1> 0> 0> 2> 0> 2> 1> 0> 2> 2> 0> 2> 1> 0> 2> 1> 2> 2> 0> 1> 0> 2> 0> 1> 2> 2> 1> 1> 0> 2> 1> 1> 2> 2> 2> 1> 1> 0> 1> 2> 1> 0> 2> 1> 2> 0> 0> 1> 2> 0> 1> 0> 0> 0> 2> 2> 0> 2> 2> 0> 2> 1> 1> 2> 0> 0> 2> 1> 2> 1> 1> 1> 0> 1> 0> 0> 1> 1> 0> 1> 2> 1> 2> 1> 0> 2> 2> 2> 2> 1> 1> 1> 1> 0> 0> 0> 0> 1> 2> 2> 1> 0> 1> 0> 2> 0> 2> 0> 2> 2> 0> 1> 0> 0> 0> 2> 1> 0> 0> 1> 1> 0> 0> 1> 1> 1> 2> 2> 0> 1> 0> 0> 1> 1> 0> 2> 2> 1> 1> 0> 2> 0> 1> 0> 0> 0> 0> 1> 0> 0> 1> 0> 1> 0> 0> 1> 0> 0> 1> 1> 0> 2> 1> 0> 1> 2> 1> 1> 0> 2> 0> 0> 2> 1> 2> 2> 0> 2> 2> 2> 1> 1> 1> 2> 1> 1> 2> 2> 1> 1> 2> 1> 1> 1> 2> 2\n",
      " Episode 4 Total Reward = -200.0\n",
      "> 1> 0> 1> 0> 0> 2> 1> 1> 2> 0> 1> 0> 1> 1> 2> 0> 1> 0> 2> 2> 0> 2> 0> 0> 2> 1> 1> 1> 1> 0> 1> 0> 2> 0> 2> 0> 0> 0> 1> 0> 0> 0> 1> 0> 0> 2> 0> 2> 2> 2> 0> 2> 0> 0> 0> 2> 1> 0> 2> 1> 1> 1> 2> 0> 0> 1> 1> 2> 1> 2> 0> 1> 1> 1> 1> 1> 2> 0> 0> 2> 1> 1> 0> 0> 0> 2> 2> 0> 1> 0> 2> 0> 0> 0> 1> 2> 2> 0> 2> 1> 0> 2> 2> 1> 0> 2> 1> 0> 2> 1> 2> 0> 0> 2> 2> 0> 1> 0> 0> 1> 2> 2> 0> 0> 0> 2> 2> 2> 1> 0> 0> 1> 2> 2> 0> 0> 1> 0> 2> 2> 0> 0> 1> 0> 0> 2> 0> 1> 2> 2> 1> 0> 0> 2> 0> 2> 0> 2> 0> 1> 2> 1> 0> 2> 0> 0> 2> 1> 0> 1> 0> 0> 0> 1> 2> 1> 1> 0> 1> 1> 0> 0> 2> 1> 2> 0> 1> 1> 0> 0> 0> 0> 2> 1> 2> 1> 2> 2> 2> 1\n",
      " Episode 5 Total Reward = -200.0\n",
      "Average Reward = -200.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "cumulative_reward = 0\n",
    "\n",
    "for e in range(1,episodes+1):\n",
    "\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        print(\">\",action, end='')\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    \n",
    "    print(f'\\n Episode {e} Total Reward = {total_reward}')\n",
    "    cumulative_reward += total_reward\n",
    "    \n",
    "average_reward = cumulative_reward/episodes \n",
    "print('Average Reward =',average_reward )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dramatic-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if env.viewer: env.viewer.close() # close the visulisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-penny",
   "metadata": {},
   "source": [
    "# 3. Use the DQN model for the CartPole example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "little-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import EpsGreedyQPolicy,LinearAnnealedPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "disabled-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_agent(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape = (1, states)))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model\n",
    "  \n",
    "model = create_agent(env.observation_space.shape[0], env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pregnant-andrew",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.4463994,  0.       ], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "loose-vector",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zuhay\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "memory = SequentialMemory(limit=100000, window_length=1)\n",
    "\n",
    "#policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', \n",
    "                              #value_max=1., value_min=.1, value_test=.05, nb_steps=10000)\n",
    "\n",
    "policy = EpsGreedyQPolicy()\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=env.action_space.n, policy=policy,\n",
    "               memory=memory, nb_steps_warmup=100, gamma=0.99, \n",
    "               enable_double_dqn= True,target_model_update= 1e-3 )\n",
    "\n",
    "dqn.compile(RMSprop(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-pocket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    3/10000 [..............................] - ETA: 5:30 - reward: -1.0000 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zuhay\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  419/10000 [>.............................] - ETA: 5:41 - reward: -1.0000"
     ]
    }
   ],
   "source": [
    "\n",
    "dqn.fit(env, nb_steps=100000, visualize=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-shooting",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "res = dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.average(res.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-builder",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
