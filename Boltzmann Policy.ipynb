{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b366d02",
   "metadata": {},
   "source": [
    "DeepQ Learning: MountainCar example in Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd3c444",
   "metadata": {},
   "source": [
    "# 1.Import the MountainCar environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "surface-launch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "demographic-cannon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of state =  2\n",
      "No of actions = 3\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "print(\"Shape of state = \", env.observation_space.shape[0])\n",
    "print(\"No of actions =\", env.action_space.n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-joseph",
   "metadata": {},
   "source": [
    "# Try some random moves to provide a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "simplified-grocery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 1> 0> 2> 0> 0> 0> 2> 1> 0> 1> 1> 0> 0> 1> 1> 2> 0> 1> 1> 1> 1> 1> 0> 0> 1> 2> 2> 2> 1> 1> 0> 0> 2> 0> 1> 2> 0> 2> 2> 0> 1> 0> 1> 2> 1> 2> 2> 0> 1> 0> 0> 2> 0> 2> 0> 1> 2> 2> 0> 1> 1> 1> 2> 1> 0> 0> 2> 0> 0> 0> 0> 1> 1> 2> 0> 0> 2> 0> 2> 0> 1> 1> 2> 0> 2> 0> 2> 0> 0> 2> 2> 0> 1> 2> 1> 0> 2> 2> 2> 2> 0> 0> 2> 1> 1> 1> 2> 1> 0> 1> 2> 1> 1> 2> 2> 2> 1> 1> 0> 0> 1> 2> 1> 0> 0> 0> 0> 0> 2> 1> 2> 1> 1> 2> 2> 2> 2> 1> 2> 1> 2> 1> 2> 2> 2> 0> 0> 2> 1> 2> 1> 0> 2> 0> 2> 1> 0> 1> 2> 0> 2> 2> 0> 2> 0> 2> 2> 0> 0> 0> 1> 2> 2> 2> 0> 1> 0> 2> 0> 0> 1> 2> 2> 1> 2> 1> 1> 1> 2> 0> 1> 2> 2> 2> 0> 1> 1> 2> 0> 2\n",
      " Episode 1 Total Reward = -200.0\n",
      "> 2> 0> 2> 1> 0> 2> 0> 2> 2> 2> 1> 0> 2> 0> 2> 2> 2> 2> 2> 0> 1> 1> 1> 2> 2> 2> 0> 1> 1> 0> 1> 0> 1> 2> 0> 2> 0> 1> 2> 0> 2> 0> 2> 2> 2> 2> 2> 2> 1> 1> 0> 1> 1> 0> 2> 2> 0> 2> 2> 1> 0> 2> 0> 0> 1> 1> 2> 2> 0> 0> 1> 0> 2> 1> 0> 1> 0> 1> 2> 2> 2> 2> 1> 0> 1> 2> 2> 2> 1> 1> 2> 0> 1> 0> 1> 1> 1> 2> 0> 0> 2> 2> 1> 0> 2> 0> 0> 1> 0> 0> 0> 2> 2> 2> 2> 2> 2> 1> 0> 2> 2> 0> 2> 2> 2> 2> 1> 0> 0> 1> 1> 2> 2> 2> 0> 0> 0> 0> 0> 1> 0> 2> 0> 1> 1> 1> 2> 0> 1> 1> 0> 1> 1> 1> 0> 0> 2> 0> 1> 2> 0> 0> 1> 2> 2> 0> 0> 2> 2> 2> 2> 2> 1> 0> 2> 2> 0> 1> 2> 2> 2> 0> 1> 1> 1> 1> 0> 1> 0> 1> 1> 0> 1> 2> 0> 2> 2> 1> 0> 1\n",
      " Episode 2 Total Reward = -200.0\n",
      "> 0> 1> 2> 2> 2> 2> 2> 1> 1> 1> 2> 0> 1> 2> 1> 2> 0> 0> 1> 0> 0> 0> 1> 0> 2> 2> 0> 2> 1> 2> 0> 1> 2> 1> 1> 0> 2> 0> 0> 0> 2> 1> 1> 2> 2> 2> 0> 0> 2> 1> 0> 2> 2> 1> 1> 0> 2> 2> 1> 2> 0> 2> 2> 2> 0> 0> 0> 0> 2> 1> 0> 1> 1> 1> 1> 1> 1> 0> 2> 0> 0> 2> 2> 0> 0> 1> 0> 2> 1> 0> 0> 2> 0> 2> 2> 0> 1> 0> 0> 2> 0> 1> 2> 0> 0> 0> 2> 0> 1> 1> 1> 1> 1> 1> 1> 1> 0> 0> 2> 1> 0> 0> 2> 1> 0> 0> 1> 0> 1> 1> 2> 2> 1> 0> 0> 1> 0> 1> 2> 0> 0> 0> 1> 1> 0> 1> 2> 0> 2> 2> 0> 0> 2> 1> 2> 1> 0> 0> 0> 1> 1> 1> 2> 0> 1> 1> 0> 2> 0> 2> 0> 1> 1> 1> 1> 2> 2> 2> 2> 1> 1> 2> 2> 0> 0> 2> 0> 0> 0> 2> 2> 0> 1> 2> 1> 1> 0> 0> 2> 0\n",
      " Episode 3 Total Reward = -200.0\n",
      "> 0> 0> 0> 0> 0> 1> 1> 2> 1> 2> 0> 0> 2> 1> 1> 1> 2> 0> 1> 1> 1> 2> 2> 0> 1> 0> 2> 1> 0> 2> 0> 1> 0> 2> 2> 0> 1> 1> 0> 2> 0> 0> 2> 1> 0> 2> 2> 2> 2> 1> 2> 1> 1> 0> 2> 1> 2> 2> 0> 1> 2> 0> 2> 2> 1> 2> 0> 2> 0> 1> 0> 0> 0> 0> 1> 1> 0> 1> 0> 0> 0> 2> 0> 2> 2> 2> 2> 1> 2> 0> 2> 2> 0> 1> 1> 1> 0> 1> 0> 1> 1> 2> 1> 2> 2> 1> 2> 0> 2> 1> 0> 2> 2> 1> 0> 0> 0> 0> 1> 0> 0> 2> 2> 0> 1> 0> 0> 1> 0> 2> 0> 1> 1> 0> 0> 1> 0> 1> 0> 0> 0> 0> 1> 2> 1> 2> 2> 2> 0> 2> 2> 2> 1> 0> 0> 0> 2> 1> 0> 2> 1> 1> 0> 2> 0> 2> 0> 0> 0> 1> 2> 0> 1> 1> 2> 1> 0> 1> 0> 0> 1> 1> 1> 0> 1> 0> 2> 1> 0> 0> 0> 2> 1> 0> 0> 0> 0> 1> 0> 0\n",
      " Episode 4 Total Reward = -200.0\n",
      "> 2> 1> 1> 0> 0> 0> 1> 1> 1> 0> 0> 1> 2> 1> 1> 0> 1> 0> 1> 1> 0> 2> 0> 2> 0> 2> 1> 0> 1> 0> 1> 2> 0> 1> 0> 2> 0> 0> 2> 1> 1> 2> 1> 1> 2> 2> 0> 1> 0> 1> 2> 1> 1> 2> 1> 1> 0> 0> 0> 2> 2> 2> 1> 2> 2> 1> 0> 1> 0> 2> 0> 2> 1> 2> 2> 1> 1> 2> 0> 0> 2> 2> 0> 0> 1> 2> 1> 1> 0> 1> 1> 0> 0> 1> 1> 0> 1> 1> 2> 1> 2> 1> 1> 2> 2> 0> 1> 0> 0> 0> 1> 2> 2> 1> 0> 1> 2> 1> 1> 1> 2> 0> 0> 2> 1> 2> 1> 1> 0> 0> 2> 2> 0> 2> 0> 1> 2> 0> 0> 2> 1> 0> 1> 1> 1> 1> 0> 1> 1> 1> 2> 2> 1> 2> 2> 2> 1> 2> 1> 0> 0> 1> 2> 0> 1> 0> 2> 2> 1> 0> 0> 0> 0> 0> 1> 1> 0> 0> 1> 1> 1> 1> 2> 0> 0> 0> 1> 1> 2> 0> 2> 0> 0> 0> 0> 1> 2> 1> 1> 2\n",
      " Episode 5 Total Reward = -200.0\n",
      "Average Reward = -200.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "cumulative_reward = 0\n",
    "\n",
    "for e in range(1,episodes+1):\n",
    "\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        print(\">\",action, end='')\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    \n",
    "    print(f'\\n Episode {e} Total Reward = {total_reward}')\n",
    "    cumulative_reward += total_reward\n",
    "    \n",
    "average_reward = cumulative_reward/episodes \n",
    "print('Average Reward =',average_reward )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a94534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dramatic-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if env.viewer: env.viewer.close() # close the visulisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-penny",
   "metadata": {},
   "source": [
    "# 3. Use the DQN model for the CartPole example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "little-difficulty",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zuhay\\source\\repos\\Deep Learning - Assignment 2\\Boltzmann Policy.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zuhay/source/repos/Deep%20Learning%20-%20Assignment%202/Boltzmann%20Policy.ipynb#ch0000008?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Dense, Flatten\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zuhay/source/repos/Deep%20Learning%20-%20Assignment%202/Boltzmann%20Policy.ipynb#ch0000008?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zuhay/source/repos/Deep%20Learning%20-%20Assignment%202/Boltzmann%20Policy.ipynb#ch0000008?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizers\u001b[39;00m \u001b[39mimport\u001b[39;00m RMSprop\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import BoltzmannQPolicy,LinearAnnealedPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_agent(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape = (1, states)))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model\n",
    "  \n",
    "model = create_agent(env.observation_space.shape[0], env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-andrew",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.43965834,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=100000, window_length=1)\n",
    "\n",
    "#policy = LinearAnnealedPolicy(BoltzmannQPolicy(), attr='tau',\n",
    "                              #value_max=1., value_min=.1, value_test=.05, nb_steps=10000)\n",
    "\n",
    "policy = BoltzmannQPolicy()\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=env.action_space.n, \n",
    "               memory=memory, nb_steps_warmup=100, gamma=0.99, policy=policy, \n",
    "               enable_double_dqn= True,target_model_update= 1e-3 )\n",
    "\n",
    "dqn.compile(RMSprop(learning_rate=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-pocket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    3/10000 [..............................] - ETA: 5:35 - reward: -1.0000 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zuhay\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  569/10000 [>.............................] - ETA: 5:30 - reward: -1.0000"
     ]
    }
   ],
   "source": [
    "\n",
    "dqn.fit(env, nb_steps=100000, visualize=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-shooting",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "res = dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.average(res.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-builder",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d0b9e7438d482284ad2eeff216662dbfad93be585d9d8ff4a3962ce208afa61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
